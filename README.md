# 혼공머신 
- 용어설명
  - 특성(feature): 생선 데이터는 [길이, 무게]의 2개의 특성으로 나타내었다.
  - 모델: 머신러닝 알고리즘이 구현된 객체를 뜻한다.모델 클래스를 추정기(estimator) 라고도함
  - 정확도: 맞힌 갯수/전체 데이터 수
  - train_set, test_set: 모델 훈련용 데이터와 이를 검증하는 테스트 데이터로 나눈다
  - data_set: input + target이 결합된 학습 데이터 셋이다.
  - 지도학습(supervised_learning): 입력과 타깃을 모델에 학습시켜 새로운 데이터를 예측하는것에 활용
  - 비지도학습(un-): 입력 데이터만 학습하기 때문에 예측하지 않음, 데이터 특징 파악에 활용
  - 강화학습(reinforcement -): 타깃없이 알고리즘의 결과로 얻는 보상을 학습에 사용
  - sampling bias: 예측할 데이터의 샘플이 골고루 섞여있지 않고 특정 타깃의 샘플을 과도하게 학습 시킬 경우 생기는 현상
  - data_preprocessing: 특성간의 기준이 달라 생기는 오차를 없애기위해 특성값의 스케일을 일정한 기준으로 맞추어주는 작업 
  - Standard_score: z-score라고도한다.특성값($x$)에서 데이터들의 평균($\mu$)을 빼고 표준편차($\sigma$)를 나눈 표준점수($z$)를 사용해서 스케일할수있다. $$z = {x-\mu\over\sigma}$$
  - Broad_casting: 넘파이 배열에서 연산을 자동으로 모든 행과 열로 확장해주는 기능
  - 회귀: 두 변수 사이의 상관관계를 분석하여 한 변수가 변할때 다른 변수는 어떻게 변하는가 파악하는 방법
  - 평균절대오차 (mean_absolute_error:MAE)
  실제값 $y$,예측값 $\hat{y}$의 오차들의 합을 평균한것
  $$MAE(y,\hat{y}) = {1\over n } \sum^n_{i=1}\vert y_i - \hat{y_i} \vert$$
  - 과대적합(overfitting): 훈련한 데이터의 $R^2$점수가 테스트용 데이터 $R^2$보다 높다면 과대적합되었다 라고한다.
  - 과소적합(underfitting): 두점수가 모두 낮거나, 테스트 점수가 너무 높다면 모델이 훈련셋에 과소적합되었다고 한다.
  - 선형회귀(linear regression): 최적의 직선,1차방정식, 선형적으로 데이터를 예측함
  - 다항회귀(polynomial-): 2차방정식, 다항식을 사용한 선형회귀, 최적의 곡선
  - 다중회귀(multiple-): 여러개의 특성을 사용하는 선형회귀이다
  => 특성이 두개가 되면 선형회귀모델은 평면을 학습한다.
  - 다중분류(multi-class_classification): 데이터를 분류할 타깃 클래스가 2개이상인 문제를 뜻한다.
  - 기울기 = 계수(coefficient) = 가중치(weight)
  - 사례기반학습: 모델 파라미터가 없음. 훈련세트를 그대로 저장하는 모델
  - 모델기반학습: 머신러닝알고리즘이 찾은 기울기계수나, 절편값을 모델 파라미터라고 부르며 최적을 모델 파라미터를 찾는것이 목표이다.
  - 특성공학(feature engineering): 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업
  - 사이킷런의 변환기(transfomer): 특성을 만들거나. 전처리하는 클래스이다.
  - 규제(regularization): 특성공학으로 특성을 마구 늘리면 완벽히 학습하여 과대적합된다. 이를 방지하는 방법이 규제이다. 선형회귀에서 계수의 크기를 줄이는 작업, 
  - 릿지(ridge): 계수를 제곱한 값을 기준으로 규제를 적용
  - 라쏘(lasso): 계수의 절댓값을 기준으로 규제를 적용 => 계수값을 0으로 만들어서 유용한 특성을 골라내는데 사용할 수 있다.
  - Hyperparameter: 수동으로 지정해줘야하는 모델클래스의 매개변수
  - boolean_indexing: 넘파이 배열을 불리언값으로된 배열과 연산이 가능하게함
  - softmax: $e\_sum=e^{z1}+e^{z2}+e^{z3} . . .$ 일때 오른쪽 각항에 e_sum을 나눠주면 $s1 = {e^{z1}\over e\_sum}, s2={e^{z2}\over e\_sum}, ...$
  s1~ sn 까지 모두 합하면 확률합은 1이 된다.
  - 점진적학습: 기존에 학습된 모델을 유지하면서 새로운 데이터를 조금씩 학습하는것
  - Stochastic_Gradient_Descent(확률적 경사 하강법): 랜덤하게 경사를 내려가는 방법=> 샘플을 랜덤하게 하나씩 골라 경사가 낮은(손실함수가 낮은,엉터리예측이 아닐 확률이 높은) 지점을 찾는 방법이다 점진적 학습이 가능하다 한세트를 전부 사용하면 1epoch 이다
  - Minibatch_Gradient_Descent: 훈련데이터의 일부를 골라 경사하강을 시행한다.
  - Batch_Gradient_Descent:
  - Loss_Function: 머신러닝 알고리즘이 엉터리인지 측정하는기준, 낮을수록 좋다
  => 연속적이어야한다 = 미분가능하다. 분류에서는 확률이 딱 정해져있지만 회귀에서는 0~1까지 연속적인 확률이 가능하다. => 손실함수를 만들 수 있다.
  - Logistic_Loss_function(=Binary_cross_entropy_loss_function):
  타깃이 1일때 -log(예측확률), 0일때 -log(1-예측확률) 으로 손실을 계산
  예측확률이 작을수록 손실이 큰양수가됨
  - Cross_entropy_loss_function: 다중분류에서 사용하는 손실함수이다. 이진분류에서 로지스틱손실함수, 회귀에서는 MSE,MAE를 사용(작을수록좋음)
  - epoch: 많은 epoch을 돌릴수록 과대적합될수도 있다. early_stopping으로 그전에 종료할수있다.
  - Hinge_loss: 머신러닝알고리즘의 하나인 Support_vector_machine의 손실함수
  - dense_layer(=fully_connected_layer): 밀집층, 완전연결층이라한다. 양쪽을 연결하는 뉴런을 모두사용하는 기본적인 인공신경망층이다.
  - one_hot_encoded: 레이블에 타깃의 해당하는 인덱스에 1, 나머지는0,categorical_crossentropy를 loss_function으로 설정
  - sparse label: 원핫인코딩은 메모리를 비효율적으로 사용하므로 0,1,2,3  등 정수형태로 레이블을 사용하는 것을뜻한다. 이때 sparse_categorical_crossentropy를 loss_function으로 설정한다.


- K-Nearest Neighbors Classifier(분류)
  - 예측할 데이터의 특성과 가까운 데이터의 타겟값을 예측으로 출력한다.(클래스 분류)
  - 학습이라기보단 단순히 데이터간의 거리를 비교해서 구하는 것이라 비효율적이다.
- K-Nearest Neighbors Regression(회귀)
  - 주변 샘플의 수치를 평균하여 타깃값을 예측
  - 회귀는 정답을 맞추는게 불가능하다. 따라서 정확도(score)를 결정계수 $R^2$ 로 평가한다 $$R^2 = 1-{(타깃-예측)^2의 합\over (타깃-평균)^2의 합}$$
  => 예측이 타깃에 가까워질수록 $R^2$는 1에 가까워진다.
- 로지스틱 회귀(logistic_regression)
  - 이름은 회귀지만 분류이다. $$z = a\times(weight)+b\times(Length)+c\times(Diagonal)+d\times(Height)+e\times(Width)+f$$
  a,b,c,e,d는 계수(가중치), 특성은 늘었지만 다중회귀와 동일한 선형방정식을 학습한다.
  - sigmoid_function: $$\phi={1\over1+e^{-z}}$$
  z가 큰 음수일때 0, 큰 양수일때 1에 가까워지게하는 함수
  - 이진분류(2개의 클래스)일때는 시그모이드함수로 z를 변환
  다중분류(다수의 클래스) 일때는 소프트맥스함수로 z값을 변환해서 확률을 출력한다.
  - L2규제, 릿지회귀와 동일한 계수의제곱을 규제 C = 1(클수록 규제 완화)
- 인공신경망(Artificial_neural_network,ANN)
  - 가장 기본적인 인공신경망은 SGD를 사용하는 로지스틱회귀와 동일하다 $$ z_{티셔츠}=w_1\times(픽셀1)+ w_2\times(픽셀2)+..+ w_{784}\times(픽셀784)+b $$
  - w=가중치,b=절편
  - 생선을 구별하면 특성(feature)이 픽셀로 바뀐것이다. 
  - 티셔츠와 바지는 가중치와 절편값을 다르게해서 구분할수있다.
  - 이렇게 계산된 z를 소프트백수 함수에 넣어서 클래스에대한 확률을 구할수 있다.
  - 클래스가 총10개이므로 $z_{10}$ 까지 존재한다.=> 출력층(output layer)
  - 뉴런(neuron) == unit :z값을 계산하는 단위, 선형계산을 한다.
  - 픽셀1~784에 해당하는 x1~x784 로 표현하고, 이를 input_layer라고 부른다.
- 딥러닝(Deep_neural_network,DNN)
  - 심층 신경망이라고한다.여러개의 층을 가지는 인공신경망이다.
  - 데이터셋이 충분히 크서 검증 점수가 안정적, 교차검증수행시간이 너무 오래걸리기때문에 검증세트만으로 검증
  - ReLu_function(렐루함수): 시그모이드는 z값이 양끝으로 갈수록 출력을 만드는데 느려짐,심층신경망에 불리,따라서 렐루함수등장 음수일경우 0, 양수일경우 입력을 그냥 통과시킴, 이미지 처리에 좋은 성능을 낸다.
- Convolution 신경망(CNN)
  - 이미지의 픽셀을 하나하나 학습하던 DNN과 다르게 픽셀들을 묶어서 값을 정하고 이를 학습해 효율적이고 효과적으로 만든것이다.
  - CNN의 뉴런은 완전연결신경망(밀집층,밀집신경망만 사용한)과는 다르다. 따라서 필터(뉴런 묶음의 개수) 또는 커널(입력에 곱하는 가중치)이라고한다, ex) 이 필터의 커널크기(3,3)
  - Feature map: 원래 픽셀을 합성곱해 나온 압축된 새로운 맵, 특성맵의 크기를 입력크기와 맞추려면 패딩을 해주어야한다.=> 모서리 픽셀값들의 합성 빈도가 낮아지기 때문이다. 
  - stride: 필터가 이동하는 크기 기본=1
  - Pooling: 특성맵의 크기를 줄이는 역할 ex) (2,2)칸의 큰값(max pooling)또는 평균값(average pooling)을 가지고 특성맵을 새로 표현, 풀링은 커널의 이동과다르게 겹치지않게 크기 만큼 이동한다.
  




  
